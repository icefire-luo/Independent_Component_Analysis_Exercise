{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5: Information Theory (Assignments)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5.1:\n",
    "\n",
    "Assume that the random variable $X$ can have two values, $a$ and $b$, as in Example 5.1. Compute the entropy as a function of the probability of obtaining $a$. Show that this is maximized when the probability is 1/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*-Reference-*:\n",
    "\n",
    "**Example 5.1** Let us comsider a random variable $X$ that can have only two values, $a$ and $b$. Denote by $p$ the probability that it has the value $a$, then the probability that it is $b$ is equal to $1-p$. The entropy of this random variable can be computed as \n",
    "\n",
    "$$\n",
    "H(X) = f(p) + f(1-p)\n",
    "$$\n",
    "\n",
    "Thus, entropy is a simple function of $p$. (It does not depend on the values $a$ and $b$.) Clearly, this function has the same properities as $f$: it is a nonnegative funciton that is zero for $p=0$ and for $p=1$, and positive for values in between. In fact, it is maximized for $p=1/2$ (this is left as an exercise). Thus, the entropy is largest when the values are both obtained with a probability of 50%. In contrast, if one of these values is obtained almost always (say, with a probability of 99.9%), the entropy of $X$ is small, since there is little randomness in the variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$-Answer-:$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5.2:\n",
    "\n",
    "Compute the entropy of $X$ in Example 5.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*-Reference-*:\n",
    "\n",
    "**Example 5.3** Consider a random variable $X$ that can have eight different values with probabilities (1/2, 1/4, 1/8, 1/16, 1/64, 1/64, 1/64, 1/64). The entropy of $X$ is 2 bits (this computation is left as an exercise to the reader). If we just coded the data in the ordinary way, we would need 3 bits for every observation. But a more intelligent way is to code frequent values with short binary strings and infrequent values with longer strings. Here, we could use the following strings for the outcomes: 0,10,110,1110,111100,111101,111110,111111. (Note that the strings can be written one after another with no spaces since they are designed so that one always knows when the seting ends.) With this encoding the *average* number of bits needed for each outcome is only 2, which is in fact equal to the entropy. So we have gained a 33% reduction of coding length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$-Answer-$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5.3:\n",
    "\n",
    "Assume $x$ has a Laplacian distribution of arbitrary variance with pdf\n",
    "\n",
    "$$\n",
    "p_x(\\xi) = \\frac{1}{\\sqrt{2}\\sigma} exp(\\frac{\\sqrt{2}}{\\sigma}\\mid\\xi\\mid)\n",
    "$$\n",
    "\n",
    "Compute the differential entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$-Answer-$:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5.4:\n",
    "\n",
    "Prove (5.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*-Referenece-*:\n",
    "\n",
    "Consider an *invertible* transformation of the random vector **x**, say (5.9)\n",
    "\n",
    "$$\n",
    "y = f(x)\n",
    "$$\n",
    "\n",
    "In this section, we show the connection between the entropy of **y** and that of **x**.\n",
    "A short, if somewhat sloppy derivation is as follows. (A more rigorous derivation is given in the Appendix.( Denote by $J\\mathbf{f}(\\mathbf{\\xi})$ the Jacobina matrix of the function **f**, i.e., the matrix of the partial derivatives of **f** at point $\\mathbf{\\xi}$. The classic relation between the density $p_y$ of **y** and the density $p_x$ of **x**, as given in Eq.(2.82), can then be formulated as (5.10)\n",
    "\n",
    "$$\n",
    "p_y(\\eta) = p_x(f^{-1}(\\eta))\\mid \\det Jf (f^{-1}(\\eta))\\mid ^{-1}\n",
    "$$\n",
    "\n",
    "Now, expressing the entropy as an expectation (5.11)\n",
    "\n",
    "$$\n",
    "H(y) = -E\\{\\log p_y(y)\\}\n",
    "$$\n",
    "\n",
    "we get (5.12)\n",
    "\n",
    "$$\n",
    "E \\{ log p_y(y) \\} \n",
    "= E \\{ log[p_x(f^{-1}(y))\\mid \\det Jf (f^{-1}(y))\\mid^{-1} \\}\n",
    "= E \\{ log[p_x(x)\\mid \\det Jf(x) \\mid^{-1} \\} \n",
    "= E \\{ \\log p_x(x) \\} - E\\{ \\log \\mid \\det Jf(x) \\mid \\}\n",
    "$$\n",
    "\n",
    "Thus we obtain the relation between the entropies as (5.13)\n",
    "\n",
    "$$\n",
    "H(y) = H(x) +E\\{log\\mid \\det Jf(x)\\mid\\}\n",
    "$$\n",
    "\n",
    "In other words, the entropy is increased in the transformation by $E\\{\\log \\det Jf(x)\\mid\\}$.\n",
    "An important special case is *the linear transformation* (5.14) \n",
    "\n",
    "$$\n",
    "y = Mx\n",
    "$$\n",
    "\n",
    "in which case we obtain (5.15)\n",
    "\n",
    "$$\n",
    "H(y) = H(x) + \\log\\mid M\\mid\n",
    "$$\n",
    "\n",
    "This also shows that differential entropy is *not scale-invariant*. If we multiply it by a scalar constant, $a$, different entropy changes as (5.16)\n",
    "\n",
    "$$\n",
    "H(ax) = H(x) + \\log \\mid a \\mid\n",
    "$$\n",
    "\n",
    "Thus, just by changing the scale, we can change the diferential entropy. This is why the scale of $x$ often is fixed before measuring its differential entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$-Answer-$:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quesiton 5.5:\n",
    "\n",
    "Prove (5.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*-Reference-*:\n",
    "\n",
    "The maximality proerty given in Section 5.3.2 shows that entropy could be used to difined a measure of non-guassianity. A measure that is zero for a gaussian variable and always non-negative can be simply obtained from differential entropy, and is called negentropy. Negentropy $J$ is defined as follows (5.24)\n",
    "\n",
    "$$\n",
    "J(x) = H(x_{gauss}) - H(x)\n",
    "$$\n",
    "\n",
    "where $x_{gauss}$ is a gaussian random vector of the same convariance matrix $\\sum$ as $x$. Its entropy can be evaluated as (5.25)\n",
    "\n",
    "$$\n",
    "H(x_{gauss}) = \\frac{1}{2} \\log \\mid \\det \\sum\\mid + \\frac{n}{2}[1+\\log 2\\pi]\n",
    "$$\n",
    "\n",
    "where $n$ is the dimension of $x$.\n",
    "\n",
    "Due to the previously mentioned maximality peroperty of the gaussian distribution, negentropy is always nonnegative. Moreover, it is zero if and only if $x$ has a gaussian distribution, since the maximum entropy distribution is unique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$-Answer-$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5.6:\n",
    "\n",
    "Show that the difinition of mutual information using Kullback-Leibler divergence is equal to the one five by entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$-Answer-$:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5.7:\n",
    "\n",
    "Compute the three first Chebyshe-Hermite polynomials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$-Answer-$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5.8:\n",
    "\n",
    "Prove (5.34). Use the orthogonality in (5.29), and in particular the fact that $H_3$ and $H_4$ are orthogonal to any second-order polynomial (prove this first!). Furthermore, use the fact that any expression involving a third-order monomial of higher-order cumulants is infinitely smaller than terms involving only sencond-order monomials (due to the assumption that the pdf is very close to gaussion)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*-Reference-*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$-Answer-$:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
