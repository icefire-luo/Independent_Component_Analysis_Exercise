{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Estimation Theory (Assignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.1: \n",
    "\n",
    "Show that:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q4.1.1.\n",
    "the maximum likelihood estimator of the variance (4.58) becomes unbiased if the estimated meand $\\hat{\\mu}_{ML}$ is replaced in (4.58) by the true one $\\mu$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*-Reference-*:\n",
    "\n",
    "The maximum likelihood estimate of the variance $\\sigma^2$ the sample variance (4.58)\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}_{ML}^{2} = \\frac{1}{T} \\sum_{j=1}^{T} [x(j)-\\hat{\\mu}_{ML}]^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$-Answer-:$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q4.1.2.\n",
    "if the mean is estimated from the observations, one must use the formula (4.6) for getting an unbiased estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*-Reference-*:\n",
    "\n",
    "The well-known formula of estimation of the variance $\\sigma^2$ of a random variable $x$ is\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}^{2} = \\frac{1}{T-1}\\sum_{j=1}^{T}[x(j)-\\hat{\\mu}]^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$-Answer-$:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.2:\n",
    "Assume that $\\hat{\\theta}_1$ and $\\hat{\\theta}_2$ are unbiased estimators of the parameter $\\theta$ having variance var($\\hat{\\theta}_1$)=$\\sigma_1^2$, var($\\hat{\\theta}_2$)=$\\sigma_2^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q4.2.1.\n",
    "\n",
    "Show that for any scalar $0\\leq\\alpha\\leq1$, the estimator $\\hat{\\theta}_3 = \\alpha\\hat{\\theta}_1 + (1 - \\alpha)\\hat{\\theta}_2$ is unbaised."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$-Answer-$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q4.2.2.\n",
    "Determine the mean-square error of $\\hat{\\theta}_3$ assuming that $\\hat{\\theta}_1$ and $\\hat{\\theta}_2$ are statistically independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$-Answer-$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q4.2.3.\n",
    "Find the value of $\\alpha$ that minimizes this mean-square error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$-Answer-$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.3:\n",
    "\n",
    "Let the scalar random variable $z$ be uniformly distributed on the interval $[0,\\theta)$. There exist $T$ independent samples $z(1),\\dots,z(T)$ from $z$. Using them, the estimate $\\hat{\\theta}=max(z(i))$ is constructed for the parameter $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q4.3.1.\n",
    "Compute the probability density function of $\\hat{\\theta}$. (*Hint*: First Construct the cumulative distribution function.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$-Answer-$:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q4.3.2.\n",
    "Is $\\hat{\\theta}$ unbiased or asymptotically unbiased?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$-Answer-$:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q4.3.3.\n",
    "What is the mean-square error $E\\{(\\hat{\\theta}-\\theta)^2\\mid\\theta\\}$ of the estimate $\\hat{\\theta}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$-Answer-$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.4:\n",
    "\n",
    "Assume that you know $T$ idependent ovdervations of a scalar quantity that is gaussian distributed with unknown mean $\\mu$ and variance $\\sigma^2$. Estimate $\\mu$ and $\\sigma^2$ using the method of moments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$-Answer-$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.5:\n",
    "\n",
    "Assume that $x(1),x(2),\\dots,x(K)$ are independent gaussian random variables having all the mean $0$ and variance $\\sigma_x^2$. Then the sum of their squares\n",
    "\n",
    "$$\n",
    "y = \\sum_{j=1}^{K}[x(j)]^2\n",
    "$$\n",
    "\n",
    "is $\\chi^2$-distributed with the mean $K\\sigma_x^2$ and variance $2K\\sigma_x^4$. Estimate the parameters $K$ and $\\theta_x^2$ using the method of moments, assuming that there exist $T$ measurements $y(1),y(2),\\dots,y(T)$ on the sum of squares $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$-Answer-$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.6:\n",
    "\n",
    "Derive the normal equations(4.37) for the least-squares criterion(4.36). Justify why these equations indeed provide the minimum of the criterion. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*-Reference-*:\n",
    "\n",
    "Because the measurement errors $v_T$ are unkown, the best that we can then do is to choose an estimator $\\hat{\\theta}$ that minimizes in some sense the effect of the errors. For mathematicall convenience, a nature choice is to consider the $least-squares$ criterion(4.36)\n",
    "\n",
    "$$\n",
    "\\varepsilon_{LS} = \\frac{1}{2}\\|\\mathbf{v}_T\\|^2 = \\frac{1}{2}(\\mathbf{x}_T - \\mathbf{H}\\mathbf{\\theta})^T(\\mathbf{x}_T - \\mathbf{H}\\mathbf{\\theta})\n",
    "$$\n",
    "\n",
    "Minimization of the criterion (4.36) with respect to the unknown parameters $\\theta$ leads to so-called $normal equations$ (4.37)\n",
    "\n",
    "$$\n",
    "(\\mathbf{H}^T\\mathbf{H})\\mathbf{\\hat{\\theta}}_{LS} = \\mathbf{H}^T\\mathbf{x}_T\n",
    "$$\n",
    "\n",
    "for determining the lease-squares estimate $\\mathbf{\\hat{\\theta}}_{LS}$ of $\\mathbf{\\theta}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$-Answer-$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.7:\n",
    "\n",
    "Assume that the measurement errors have zero mean: $E\\{\\mathbf{v}_T\\}=0$, and that the covariance matrix of the measurement erros is $\\mathbf{C}_{\\mathbf{v}} = E\\{\\mathbf{v}_T\\mathbf{v}_T^T\\}$. Consider the  properties of the least-squares estimator $\\hat{\\mathbf{\\theta}}_{LS}$ in (4.38)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*-Reference-*:\n",
    "\n",
    "Minimization of the criterion (4.36) with respect to the unknown parameters $\\theta$ leads to so-called $normal equations$ (4.37)\n",
    "\n",
    "$$\n",
    "(\\mathbf{H}^T\\mathbf{H})\\mathbf{\\hat{\\theta}}_{LS} = \\mathbf{H}^T\\mathbf{x}_T\n",
    "$$\n",
    "\n",
    "for determining the lease-squares estimate $\\mathbf{\\hat{\\theta}}_{LS}$ of $\\mathbf{\\theta}$. It is often most convenient to solve $\\hat{\\mathbf{\\theta}}_{LS}$ from these linear equations. However, because we assumed we that the matrix $\\mathbf{H}$ has full rank, we can explicitly solve the normal equations, getting (4.38)\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{\\theta}}_{LS} = (\\mathbf{H}^T\\mathbf{H})^{-1}\\mathbf{H}^T\\mathbf{x}_T = \\mathbf{H}^{+}\\mathbf{x}_T\n",
    "$$\n",
    "\n",
    "where $\\mathbf{H}^+ = (\\mathbf{H}^T\\mathbf{H})^{-1}\\mathbf{H}^T$ is the *pseudoinverse* of $\\mathbf{H}$ (assuming that $\\mathbf{H}$ has maximal rank $m$ and more rows that columns: $T > m$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q4.7.1.\n",
    "Show that the estimator $\\hat{\\mathbf{\\theta}}_{LS}$ is unbiased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$-Answer-$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4.7.2.\n",
    "Compute the error covariance matrix $\\mathbf{C}_{\\tilde{\\mathbf{\\theta}}}$ defined in (4.19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*-Reference-*:\n",
    "Still another useful measure of the quality of an estimator is given by the covariance matrix of the estimation error (4.19)\n",
    "\n",
    "$$\n",
    "\\mathbf{C}_{\\tilde{\\mathbf{\\theta}}} = E\\{\\tilde{\\mathbf{\\theta}}\\tilde{\\mathbf{\\theta}}^T\\} = E\\{(\\mathbf{\\theta} - \\hat{\\mathbf{\\theta}})(\\mathbf{\\theta} - \\hat{\\mathbf{\\theta}})^T\\}\n",
    "$$\n",
    "\n",
    "It measures the errors of individual parameter estimates, while the mean-square error is an overall scalar error meausre for all the parameter estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$-Answer-$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q4.7.3.\n",
    "Compute $\\mathbf{C}_{\\tilde{\\mathbf{\\theta}}}$ when $\\mathbf{C_v} = \\sigma^2\\mathbf{I}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$-Answer-$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.8:\n",
    "\n",
    "Consider line fitting using the linear least-squares method. Assume that you know $T$ measurements $x(1),x(2),\\dots,x(T)$ on the scalar quantity $x$ made, respectively, at times (or argument values) $t(1),t(2),\\dots,x(T)$. The task is to fit the line \n",
    "\n",
    "$$\n",
    "x = a_0 + a_1t\n",
    "$$\n",
    "\n",
    "to these measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q4.8.1.\n",
    "Construct the normal equations for this problem using the standard linear least-squares method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$-Answer-$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q4.8.2.\n",
    "Assume that the sampling interval $\\Delta t$ is constant and has been scaled so theat the measurement times are integers $1,2,\\dots,T$. Solve the normal equations in this important special case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$-Answer-$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.9:\n",
    "\n",
    "\\* Consider the quivalence of the generalized least-squares and linear unbiased minimum mean-square estimators. Show that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q4.9.1.\n",
    "The optimal solution minimizing the generalized least-squares criterion(4.45) is\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{\\theta}}_{WLS} = (\\mathbf{H}^T\\mathbf{W}\\mathbf{H})^{-1}\\mathbf{H}^T\\mathbf{W}\\mathbf{x}_T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*-Reference-*:\n",
    "\n",
    "The least-squares problem can be generaizedby adding a sysmmetric and positive definite weighting matrix $\\mathbf{W}$ to be criterion(4.36).\n",
    "The weighted criterion becomes\n",
    "\n",
    "$$\n",
    "\\epsilon_{WLS} = (\\mathbf{x}_T - \\mathbf{H}\\mathbf{\\theta})^T\\mathbf{W}(\\mathbf{x}_T - \\mathbf{H}\\mathbf{\\theta})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$-Answer-$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q4.9.2.\n",
    "An unbiased linear mean-square estimator $\\hat{\\mathbf{\\theta}}_{MSE}= \\mathbf{L}\\mathbf{x}_T$ satisfies the condition $\\mathbf{L}\\mathbf{H} = \\mathbf{I}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$-Answer-$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q4.9.3.\n",
    "The mean-squares error can be written in the form\n",
    "\n",
    "$$\n",
    " \\epsilon_{MSE} = E\\{ \\| \\mathbf{\\theta} - \\hat{\\mathbf{\\theta}} \\| ^2 \\mid \\theta \\} = trace(\\mathbf{L}\\mathbf{C_v}\\mathbf{L}^T)\n",
    " $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$-Answer-:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q4.9.4.\n",
    "Minimization of the preceding criterion $\\epsilon_{MSE}$ unde the constraint $\\mathbf{L}\\mathbf{H}=\\mathbf{I}$ leads to the BLUE estimator (4.46)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*-Reference-*:\n",
    "\n",
    "It turns out that a natural, optimal choice for the weighting matrix $\\mathbf{W}$ is the inverse of the covariance matrix of the measurement errors (noise) $\\mathbf{W} = \\mathbf{C}^{-1}_\\mathbf{v}$. This is because for this  choice the resulting generalized least-squares estimator(4.46)\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{\\theta}}_{WLS} = (\\mathbf{H}^T\\mathbf{C}^{-1}_\\mathbf{v}\\mathbf{H})^{-1}\\mathbf{H}^T\\mathbf{C}^{-1}_\\mathbf{v}\\mathbf{x}_T\n",
    "$$\n",
    "\n",
    "also minimizes the mean-square estimation error $ \\epsilon_{MSE} = E\\{ \\| \\mathbf{\\theta} - \\hat{\\mathbf{\\theta}} \\| ^2 \\mid \\theta \\}$. Here it is assumed that the estimator $\\hat{\\mathbf{\\theta}}$ is linear and unbiased. The estimator (4.46) is often referred to as the *best linear unbiased estimator (BLUE) or Gauss-Markov estimator*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$-Answer-$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.10:\n",
    "\n",
    "For a fixed amount of gas, the following connection holds between the pressure $P$ and the volume $V$:\n",
    "\n",
    "$$\n",
    "PV^\\gamma = c\n",
    "$$\n",
    "\n",
    "where $\\gamma$ and $c$ are constants. Assume that we know $T$ pairs of measurements $(P_i,V_i)$. We want to estimate the parameters $\\gamma$ and $c$ using the *linear least-squares* method. Express the situation in the form of a matrix-vector model and explain how the estimates are computed (you need not compute the exact solution)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$-Answer-$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.11:\n",
    "\n",
    "Let the probability density function of a scalar-valued random variable $z$ be\n",
    "\n",
    "$$\n",
    "p(z \\mid \\theta)=\\theta^2ze^{-\\theta z}, \\quad z \\geq 0, \\theta > 0\n",
    "$$\n",
    "\n",
    "Determine the maximum likelihood estimate of the parameter $\\theta$. There are available $T$ independent measurements $z(1),\\dots,z(T)$ on $z$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$-Answer-$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.12:\n",
    "\n",
    "In a signal processing application five sensors placed mutually according to a cross pattern yield, respectively, the measurements $x_0,x_1,x_2,x_3,$and $x_4$, that can be collected to the measurement vector $x$. The measurements are quantized with 7 bits accuracy so that their values are integers in the interval $0,\\dots,127$. The joint density $p(x \\mid \\theta)$ of the measuremenets is a multinomial density that depends on the unknown parameter $\\theta$ as follows:\n",
    "\n",
    "$$\n",
    "p(x \\mid \\theta) = k(x)(1/2)^{x_0}(\\theta/4)^{x1}(1/4-\\theta/4)^{x2}(1/4-\\theta/4)^{x3}(\\theta/4)^{x4}\n",
    "$$\n",
    "\n",
    "where the scaling term\n",
    "\n",
    "$$\n",
    "k(x) = \\frac{(x_0 + x_1 + x_2 + x_3 + x_4)!}{x_0!x_1!x_2!x_3!x_4!}\n",
    "$$\n",
    "\n",
    "Determine the maximum likelihood estimate of the parameter $\\theta$ in terms of the measurement vector $x$. (Here, you can here treat the individual measurements in a similar manner as mutually independent scalar measurements.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$-Answer-$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.13:\n",
    "\n",
    "Consider the sum $z=x_1 + x_2 + \\dots + x_K$, where the scalar random variables $x_i$ are statistically indepedent and gaussian, each having the same mean $0$ and variance $\\sigma^2_x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q4.13.1.\n",
    "Construct the maximum likelihood estimate for the number $K$ of the terms in the sum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$-Answer-$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Q4.13.2.\n",
    "Is this estimate unbiased?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$-Answer-$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.14:\n",
    "\n",
    "\\* Consider direct evaluation of the Wiener filter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q4.14.1.\n",
    "Show that the mean-square filtering error (4.78) can be evaluated to the form (4.79)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*-Reference-*:\n",
    "\n",
    "In *Wiener filtering*, the goal is to determine the linear filter $y= w ^Tz$ that minimizes the mean-square error (4.78)\n",
    "\n",
    "$$\n",
    "\\epsilon _{MSE} = E\\{(y - d)^2\\}\n",
    "$$\n",
    "\n",
    "between the *desired response d* and the output $y$ of the filter. Inserting $y=w^Tz$ into above equation and evaluating the expectation yields (4.79)\n",
    "\n",
    "$$\n",
    "\\epsilon_{MSE} = \\mathbf{w}^T\\mathbf{R_z}\\mathbf{w} - 2\\mathbf{w}^T\\mathbf{r_{zd}} + E{d^2} \n",
    "$$\n",
    "\n",
    "Here $\\mathbf{R_z}=E\\{zz^T\\}$ is the data correlation matrix, and $\\mathbf{r_{zd}} = E\\{zd\\}$ is the crosscorrelation vector between the data vector $z$ and the desired response $d$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$-Answer-$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q4.14.2.\n",
    "What is the minimum mean-square error given by the Wiener estimate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$-Answer-$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.15:\n",
    "\n",
    "The random variables $x_1,x_2$, and a third, related random variable $y$ are jointly distributed. Define the random vector\n",
    "\n",
    "$$\n",
    "z = [ y, x_1, x_2 ]^T\n",
    "$$\n",
    "\n",
    "It is known that $z$ has the mean vector $m_z$ and the covariance matrix $\\mathbf{C_z}$ given by \n",
    "\n",
    "$$\n",
    "\\mathbf{m}_z = \\begin{bmatrix}\n",
    "                 1/4 \\\\ 1/2 \\\\ 1/2 \n",
    "               \\end{bmatrix},\n",
    "\\quad\n",
    "\\mathbf{C}_z = \\frac{1}{10} \\begin{bmatrix}\n",
    "                             7 &  1 &  1 \\\\\n",
    "                             1 &  3 & -1 \\\\\n",
    "                             1 & -1 &  3\n",
    "                            \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Find the optimum linear mean-square estimate of $y$ based on $x_1$ and $x_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$-Answer-$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.16:\n",
    "\n",
    "\\* Assume that you know $T$ data vectors $z(1),z(2),\\dots,z(T)$ and their corresponding desired responses $d(1),d(2),\\dots,d(T)$. Standard estimates of the correlation matrix and the cross-correlation vector needed in Wiener filtering are (4.87)\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{R}}_\\mathbf{z} = \\frac{1}{t} \\sum_{i=1}^{T}\\mathbf{z}(i)\\mathbf{z}(i)^T\n",
    ",\\quad \\quad\n",
    "\\hat{\\mathbf{r}}_{\\mathbf{zd}} = \\frac{1}{T} \\sum_{i=1}^{T}\\mathbf{z}(i)d(i)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q4.16.1.\n",
    "Express the estimates (4.87) in matrix form and show that when they are used in Wiener filter $\\hat{\\mathbf{w}}_{MSE} = \\mathbf{R}_{\\mathbf{z}}^{-1}\\mathbf{r}_{\\mathbf{z}d}$ instead of the true values, the filter coincides with a least-squares solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$-Answer-$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q4.16.2.\n",
    "What is the discrete data model correspoding to this least-squares estimator?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$-Answer-$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.17:\n",
    "\n",
    "\\* The joint density function of the random variables $x$ and $y$ is given by \n",
    "\n",
    "$$\n",
    "p_{xy}(x,y) = 8xy, \\quad 0 \\leq y \\leq x \\leq 1,\n",
    "$$\n",
    "\n",
    "and $p_{xy}(x,y) = 0$ outside the region defined above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q4.17.1.\n",
    "Find and sketch the condigitonal density $p_{y \\mid x}(y \\mid x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$-Answer-$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q4.17.2.\n",
    "Compute the MAP (maximum a posteriori) estimate of $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$-Answer-$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q4.17.3.\n",
    "Compute the optimal mean-square error estimate of $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$-Answer-$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.18:\n",
    "\n",
    "\\* Suppose that a scalar random variable $y$ is of the form $y = z + v$, where the pdf of $v$ is $p_v(t) = t/2$ on the interval $[0,2]$, and the pdf of $z$ is $p_z(t)=2t$ on the interval $[0,1]$. Both the densities are zero elsewhere. There is available a single measurement value $y=2.5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q4.18.1.\n",
    "Compute the maximum likelihood estimate of $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$-Answer-$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q4.18.2.\n",
    "Compute the MAP (maximum a posteriori) estimate of $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$-Answer-$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q4.18.3.\n",
    "Compute the minimum mean-square estimate of $y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$-Answer-$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questioin 4.19:\n",
    "\n",
    "\\* Consider the MAP estimator (4.84) of the mean $\\mu$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*-Reference-*:\n",
    "\n",
    "Using the preceding information, it is straightforward to form the likelihood equation for the MAP estimator $\\hat{\\mu}_{MAP}$  and solve it. The solution is (4.84) \n",
    "\n",
    "$$\n",
    "\\hat{\\mu}_{MAP} = \\frac{\\sigma_{\\mu}^2}{\\sigma_{x}^2 + T\\sigma_{\\mu}^2} \\sum_{j=1}^{T} x(j)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q4.19.1\n",
    "Derive the estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$-Answer-$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q4.19.2.\n",
    "Express the estimator in recursive form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$-Answer-$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
