{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Gradients and Optimization Methods (Assignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3.1:\n",
    "\n",
    "Show that the Jacobian matrix of the gradient vector $\\frac {\\partial g}{\\partial \\mathbf{w}}$ with respect to $\\mathbf{w}$ is equal to the Hessian of $g$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$-Answer-:$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3.2:\n",
    "\n",
    "The trace of an $m \\times m$ square matrix $\\mathbf{W}$ is defined as the sum of its diagonal elements $\\sum_{i=1}^{m}\\omega_{ii}$. Compute its matrix gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$-Answer-$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3.3\n",
    "\n",
    "Show that the gradient of trace($\\mathbf{W}^T\\mathbf{MW}$) with respect to $\\mathbf{W}$, where $\\mathbf{W}$ is an $m \\times n$ matrix and $\\mathbf{M}$ is an $m \\times m$ matrix, is equal to $\\mathbf{MW} + \\mathbf{M}^T\\mathbf{W}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$-Answer-$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3.4\n",
    "\n",
    "Show that $\\frac {\\partial} {\\partial\\mathbf{W}} \\log |\\det\\mathbf{W}|=(\\mathbf{W}^T)^{-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$-Answer-$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3.5:\n",
    "\n",
    "Consider the $2 \\times 2$ matrix\n",
    "\n",
    "$$\n",
    "\\mathbf{W} = \\begin{pmatrix}\n",
    "                a & b \\\\\n",
    "                c & d \\\\\n",
    "             \\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q3.5.1.\n",
    "Compute the cofactors with respect to the first column, compute the determinant, the adjoint matrix, and the inverse of $\\mathbf{W}$ as functions of $a,b,c,d$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$-Answer-$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q3.5.2.\n",
    "\n",
    "Verify in this special case that $\\frac {\\partial} {\\partial\\mathbf{W}} \\log |\\det\\mathbf{W}|=(\\mathbf{W}^T)^{-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$-Answer-$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3.6:\n",
    "\n",
    "Consider cost function $\\jmath (\\mathbf{w}) = G(\\mathbf{w}^T\\mathbf{x})$ where we can assume that $\\mathbf{x}$ is a constant vector. Assume that the scalar function $G$ is twice differentiable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q3.6.1. \n",
    "\n",
    "Compute the gradient and Hessian of $\\jmath(\\mathbf{x})$ in the general case and in the cases that $G(t)=t^4$ and $G(t)=\\log\\cosh(t)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$-Answer-$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q3.6.2.\n",
    "\n",
    "Consider maximizing this funciton under the constrain that $\\|\\mathbf{w}\\|=1$. Formulate the Lagrangian, its gradient (with respect to $\\mathbf{w}$), its Hessian, and the Newton method for maximizing the Lagrangian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$-Answer-$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3.7:\n",
    "\n",
    "Let $p(.)$ be a differentiable scalar function, $\\mathbf{x}$ a constant vector, and $\\mathbf{W} = (\\mathbf{w}_1 \\dots  \\mathbf{w}_n)$ an $m \\times n$ matrix with columns $\\mathbf{w}_i$. Consider the cost function\n",
    "\n",
    "$$\n",
    "\\jmath(\\mathbf{W}) = \\sum_{i=1}^{n}\\log p(u_i)\n",
    "$$\n",
    "\n",
    "where $u_i = \\mathbf{x}^T\\mathbf{w}_i$. Show that $\\frac{\\partial}{\\partial\\mathbf{W}}\\jmath(\\mathbf{W}) = -\\varphi(\\mathbf{u})\\mathbf{x}^T$ where $\\mathbf{u}$ is the vector with elements $u_i$ and $\\varphi(\\mathbf{u})$ is a certain function, defined element by element. Give the form of this function. (*Note*: This matrix gradient is used in the maximum liklihood approach to ICA, discussed in Chapter 9.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$-Answer-$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3.8:\n",
    "\n",
    "Consider a general stochastic on-line ICA learning rule\n",
    "\n",
    "$$\n",
    "\\Delta \\mathbf{W} \\propto [\\mathbf{I} - g(\\mathbf{y})\\mathbf{y}^T] \\mathbf{W}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{y} = \\mathbf{Wx}$ and $g$ is a nonlinear function. Formulate\n",
    "\n",
    "  (a) the corresponding batch learning rule,\n",
    "\n",
    "  (b) the averaged differential equation.\n",
    "\n",
    "Consider a stationary point of (a) and (b). Show that if $\\mathbf{W}$ is such that the elements of $\\mathbf{y}$ are zero-mean and independent, then $\\mathbf{W}$ is a stationary point.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$-Answer-$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3.9:\n",
    "\n",
    "Assume that we want to maximize a function $F(\\mathbf{w})$ on the unit sphere, i.e., under the constraint $\\|\\mathbf{w}\\|=1$. Prove that at the maximum, the gradient of $F$ must point in the same direction as $\\mathbf{w}$. In other words, the gradient must be equal to $\\mathbf{w}$ multiplied by a scalar constant. Use the Lagrangian method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$-Answer-$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
